{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Models\n","Looking for `torch` and `keras` pre-trained models.\n","1. Vision Transformer - Both\n","2. VGG19 - Both\n","3. ResNet50 - Both\n","4. Other Models - MobileNetV2, AlexNet, DenseNet201, DenseNet121, InceptionV3, ResNet50V2, InceptionResNetV2, and Xception"],"metadata":{"id":"VaTVNaChit57"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"QO6V2FE6gGbI","executionInfo":{"status":"ok","timestamp":1742668515466,"user_tz":420,"elapsed":28514,"user":{"displayName":"David Pham","userId":"09686704807332382354"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import models, transforms, datasets\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import ImageFolder\n","from tqdm import tqdm\n","\n","from PIL import Image\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_score, recall_score, f1_score"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"uGvcWkFSs2uS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742668568899,"user_tz":420,"elapsed":33247,"user":{"displayName":"David Pham","userId":"09686704807332382354"}},"outputId":"5ce375dc-ae8e-4cbb-a9a4-724b7e62d1a8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/AJL Team 2/bttai-ajl-2025'"],"metadata":{"id":"yveIx6des6M1","executionInfo":{"status":"ok","timestamp":1742668568900,"user_tz":420,"elapsed":5,"user":{"displayName":"David Pham","userId":"09686704807332382354"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Data Augmentation\n","Read the [paper](https://arxiv.org/abs/2104.09957) linked in the Kaggle competition, we can pull some ideas from what they did."],"metadata":{"id":"eU8y-dgsoC9i"}},{"cell_type":"code","source":["data_transforms = transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(30),\n","        transforms.ToTensor()\n","])"],"metadata":{"id":"3XPNp94LIoxq","executionInfo":{"status":"ok","timestamp":1742668568901,"user_tz":420,"elapsed":4,"user":{"displayName":"David Pham","userId":"09686704807332382354"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Load Datasets"],"metadata":{"id":"BGOGyXfDiJJt"}},{"cell_type":"code","source":["train_root_dir = os.path.join(path, 'train', 'train')\n","train_csv_path = os.path.join(path, 'train.csv')"],"metadata":{"id":"2KDVFA9nJ1Ss","executionInfo":{"status":"ok","timestamp":1742668568901,"user_tz":420,"elapsed":4,"user":{"displayName":"David Pham","userId":"09686704807332382354"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Do the HAM10000 DatasetFirst"],"metadata":{"id":"DraNZWafPUIv"}},{"cell_type":"code","source":["from torch.utils.data import Dataset,  DataLoader\n","import torchvision.transforms as transforms\n","import os\n","from PIL import Image\n","\n","class HAM100000(Dataset):\n","    def __init__(self, csv_file, part1_dir, part2_dir, transform):\n","      self.data = pd.read_csv(csv_file)[['image_id', 'dx']]\n","      self.part1_dir = part1_dir\n","      self.part2_dir = part2_dir\n","      self.transform = transform\n","      self.split_index = 29305\n","      self.label_map = {  # Convert labels to integers\n","            'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3,\n","            'mel': 4, 'nv': 5, 'vasc': 6\n","        }\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_filename = self.data.iloc[idx, 0]\n","        label = self.label_map[self.data.iloc[idx, 1]]\n","        fileSplit = int(self.data.iloc[idx, 0].split('_')[1])\n","        if fileSplit <= self.split_index:\n","          img_path = os.path.join(self.part1_dir, img_filename)+'.jpg'\n","        else:\n","          img_path = os.path.join(self.part2_dir, img_filename) +'.jpg'\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","          image = self.transform(image)\n","\n","        return image, label"],"metadata":{"id":"8FrF10AayZBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load ImageFolder\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor()\n","])\n","\n","path_1 = \"/content/drive/MyDrive/AJL Team 2/Ham10000/HAM10000_images_part_1\"\n","path_2 = \"/content/drive/MyDrive/AJL Team 2/Ham10000/HAM10000_images_part_2\"\n","csvpath = \"/content/drive/MyDrive/AJL Team 2/Ham10000/HAM10000_metadata.csv\"\n","\n","dataset_Ham10000 = HAM100000(csvpath, path_1, path_2, transform)"],"metadata":{"id":"QebhFrZ3ybCP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import random_split\n","#make dataloaders into train and validation split\n","train_size = int(0.8 * len(dataset_Ham10000))\n","val_size = len(dataset_Ham10000) - train_size\n","train_dataset_HAM10000, val_dataset_HAM10000 = random_split(dataset_Ham10000, [train_size, val_size])"],"metadata":{"id":"rmXmWRbcyccK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load pretrained ViT model\n","model = models.vit_b_16(weights=\"IMAGENET1K_V1\")"],"metadata":{"id":"9G6nSIPDyelt","executionInfo":{"status":"ok","timestamp":1742604544628,"user_tz":420,"elapsed":3828,"user":{"displayName":"Fawn Gonick","userId":"03218061954015853305"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b543e69a-4f83-405b-e9c6-4c1882f8b2ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n","100%|██████████| 330M/330M [00:02<00:00, 160MB/s]\n"]}]},{"cell_type":"code","source":["# Replace classifier head (FC layer)\n","model.heads.head = nn.Linear(model.heads.head.in_features, 7)\n","\n","# Move model to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"],"metadata":{"id":"zflowyPUz84m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Freeze all parameters first\n","\"\"\"\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","for name, param in model.named_parameters():\n","    if \"encoder.layers.encoder_layer_\" in name:  # Match correct layer naming\n","        layer_num = int(name.split(\"encoder.layers.encoder_layer_\")[-1].split(\".\")[0])  # Extract layer index\n","        if layer_num >= 8:  # Unfreeze last 4 blocks\n","            param.requires_grad = True\n","        elif \"heads.head\" in name:  # Unfreeze final layer head\n","         param.requires_grad = True\"\"\"\n","         #dont freeze anything\n"],"metadata":{"id":"eXnS8Tke0PGu","executionInfo":{"status":"ok","timestamp":1742604576479,"user_tz":420,"elapsed":17,"user":{"displayName":"Fawn Gonick","userId":"03218061954015853305"}},"colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"73fc633e-f45d-4f80-de9d-704d4b1d4fa5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfor param in model.parameters():\\n    param.requires_grad = False\\n\\nfor name, param in model.named_parameters():\\n    if \"encoder.layers.encoder_layer_\" in name:  # Match correct layer naming\\n        layer_num = int(name.split(\"encoder.layers.encoder_layer_\")[-1].split(\".\")[0])  # Extract layer index\\n        if layer_num >= 8:  # Unfreeze last 4 blocks\\n            param.requires_grad = True\\n        elif \"heads.head\" in name:  # Unfreeze final layer head\\n         param.requires_grad = True'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["!pip install torcheval"],"metadata":{"id":"fT19tQgP2OMe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742668572709,"user_tz":420,"elapsed":3811,"user":{"displayName":"David Pham","userId":"09686704807332382354"}},"outputId":"0bc340f8-df79-4498-d2b0-71a28961b17e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torcheval\n","  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torcheval) (4.12.2)\n","Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torcheval\n","Successfully installed torcheval-0.0.7\n"]}]},{"cell_type":"code","source":["import torch\n","from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score\n","from tqdm import tqdm\n","\n","# Function to train and validate model\n","def train_model(model, train_data, val_data, batch_size, criterion, optimizer, num_classes, device, save_path, num_epochs=10, scheduler=None):\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","    model.to(device)\n","\n","    best_val_f1 = float('-inf')  # Initialize best F1 score as very low\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        scaler = torch.amp.GradScaler(device)\n","\n","        for images, labels in tqdm(train_loader):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            with torch.cuda.amp.autocast():\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            running_loss += loss.item()\n","\n","        avg_train_loss = running_loss / len(train_loader)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        accuracy_metric = MulticlassAccuracy(num_classes=num_classes, device=device)\n","        f1_metric = MulticlassF1Score(num_classes=num_classes, average=\"macro\", device=device)\n","\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","                # Update metrics\n","                accuracy_metric.update(outputs, labels)\n","                f1_metric.update(outputs, labels)\n","\n","        avg_val_loss = val_loss / len(val_loader)\n","        val_accuracy = accuracy_metric.compute().item()\n","        val_f1 = f1_metric.compute().item()\n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n","\n","        # Save best model based on F1 score\n","        if val_f1 > best_val_f1:\n","            best_val_f1 = val_f1\n","            save_model(model, save_path)\n","\n","        # Step the scheduler if provided\n","        if scheduler:\n","            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n","                scheduler.step(avg_val_loss)  # ReduceLROnPlateau needs validation loss\n","            else:\n","                scheduler.step()  # Other schedulers just step normally\n","\n","# Function to save model\n","def save_model(model, path=\"/content/drive/MyDrive/AJL Team 2/HAM1000model.pth\"):\n","    torch.save(model.state_dict(), path)\n","    print(f\"Model saved to {path}\")\n"],"metadata":{"id":"WcS6nlJu2OjQ","executionInfo":{"status":"ok","timestamp":1742668574366,"user_tz":420,"elapsed":1658,"user":{"displayName":"David Pham","userId":"09686704807332382354"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Loss and Optimizer\n","#tbh probably just gonna have this run until it break or something\n","criterion = nn.CrossEntropyLoss()\n","#give it an optimizer\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","\n","train_model(model, train_dataset_HAM10000, val_dataset_HAM10000, 64, criterion, optimizer, 7, device, '/content/drive/MyDrive/AJL Team 2/VITLHAM1000model.pth', 10)"],"metadata":{"id":"aR_UFCisIxrf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## After pretrain on the augmented dataset"],"metadata":{"id":"KAZ5nH7H2xto"}},{"cell_type":"code","source":["#create dataloader for this\n","from torchvision.datasets import ImageFolder\n","class ImageFolderWithColor(Dataset):\n","    def __init__(self, image_folder):\n","        self.image_folder = image_folder\n","        self.image_paths, self.labels = zip(*self.image_folder.samples)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        label = self.labels[idx]\n","        # Extract filename from the path\n","        filename = os.path.basename(img_path)\n","\n","        # Load image and apply transformations\n","        image, _ = self.image_folder[idx]\n","\n","        return image, label"],"metadata":{"id":"TT66WF7z2hrb","executionInfo":{"status":"ok","timestamp":1742668574368,"user_tz":420,"elapsed":3,"user":{"displayName":"David Pham","userId":"09686704807332382354"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#add in a bunch of transforms\n","from torchvision.transforms.v2 import ElasticTransform\n","\"\"\"transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(4/5, 5/4)),  # Keeps most of the lesion\n","    transforms.RandomHorizontalFlip(p=0.7),  # Flip images randomly\n","    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.3, hue=0.05),  # Preserve color, minimal hue change\n","    transforms.RandomRotation(degrees=10),  # Small rotations\n","    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1, 0.5)),  # Slight blur\n","    transforms.ToTensor()  # Convert to tensor (NO NORMALIZATION, does not make sense for this task!)\n","])\"\"\"\n","#different transformations\n","transform = transforms.Compose([\n","    # Random crop while maintaining lesion structure\n","    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(4/5, 5/4)),\n","\n","    # Horizontal flip\n","    transforms.RandomHorizontalFlip(p=0.7),\n","\n","    # Color jitter for brightness, contrast, saturation, hue\n","    transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),\n","\n","    # Random rotation\n","    transforms.RandomRotation(degrees=20),\n","\n","    # Slight Gaussian blur\n","    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.8)),\n","\n","    # Elastic transformation (simulates deformations, helps robustness)\n","    ElasticTransform(alpha=30.0),\n","\n","    # Convert to tensor (NO NORMALIZATION here if it's task-specific)\n","    transforms.ToTensor()\n","])\n","\n","#we are just going to work with the original dataset for now\n","path = \"/content/drive/MyDrive/AJL Team 2/bttai-ajl-2025/train/train\"\n","# Load dataset from the correct directory\n","dataset_pre = ImageFolder(root=path, transform=transform)\n","\n","# Create the custom dataset\n","dataset = ImageFolderWithColor(dataset_pre)\n","\n","# Load into DataLoader\n","data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"-3y4_b3xiOMj","executionInfo":{"status":"ok","timestamp":1742668583631,"user_tz":420,"elapsed":9263,"user":{"displayName":"David Pham","userId":"09686704807332382354"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import random_split\n","# Split into training and validation sets\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"],"metadata":{"id":"JnJAV4cMiQpd","executionInfo":{"status":"ok","timestamp":1742668583650,"user_tz":420,"elapsed":2,"user":{"displayName":"David Pham","userId":"09686704807332382354"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["##load in pretrained model then replace the fc with the correct size\n","finalModel = models.vit_b_16(weights=\"IMAGENET1K_V1\")\n","\n","# Replace classifier head (FC layer)\n","finalModel.heads.head = nn.Linear(finalModel.heads.head.in_features, 7)\n","\n","# Move model to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","finalModel = finalModel.to(device)\n","\n","# Load the saved weights\n","finalModel.load_state_dict(torch.load(\"/content/drive/MyDrive/AJL Team 2/VITHAM1000model.pth\", map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n","\n","finalModel.heads.head = nn.Linear(finalModel.heads.head.in_features, 21) #switch out to correct size final layer"],"metadata":{"id":"TOukhBzhiUKI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742668596541,"user_tz":420,"elapsed":12888,"user":{"displayName":"David Pham","userId":"09686704807332382354"}},"outputId":"81e9625b-9814-4b45-fd7a-76454359efa6"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n","100%|██████████| 330M/330M [00:01<00:00, 180MB/s]\n"]}]},{"cell_type":"code","source":["# Freeze all parameters first\n","for param in finalModel.parameters():\n","    param.requires_grad = False\n","\n","for name, param in finalModel.named_parameters():\n","    if \"encoder.layers.encoder_layer_\" in name:  # Match correct layer naming\n","        layer_num = int(name.split(\"encoder.layers.encoder_layer_\")[-1].split(\".\")[0])  # Extract layer index\n","        if layer_num >= 4:  # Unfreeze last 4 blocks\n","            param.requires_grad = True\n","        elif \"heads.head\" in name:  # Unfreeze final layer head\n","         param.requires_grad = True\n","\n","# Move model to GPU\n","finalModel.to(device)"],"metadata":{"id":"SA2Sx7woiXO6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742668596549,"user_tz":420,"elapsed":6,"user":{"displayName":"David Pham","userId":"09686704807332382354"}},"outputId":"01ef1799-4758-42bd-8313-5f455df70dd7"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VisionTransformer(\n","  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","  (encoder): Encoder(\n","    (dropout): Dropout(p=0.0, inplace=False)\n","    (layers): Sequential(\n","      (encoder_layer_0): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_1): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_2): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_3): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_4): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_5): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_6): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_7): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_8): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_9): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_10): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_11): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (heads): Sequential(\n","    (head): Linear(in_features=768, out_features=21, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["#now retrain the original model using the data we were given and see how it does\n","\n","#reduce lr if same for 2 epochs\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","#give it an optimizer, adding ab it of weight decay\n","#seems to overfitt give it a bit more weigth decay\n","optimizer = torch.optim.Adam(finalModel.parameters(), lr=1e-4, weight_decay=5e-5)\n","#use a learning rate schedular\n","\n","#give patience of 3 and a minimum lr so that it always keeps learning\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, min_lr=1e-7)\n","#give a learning rate schedular\n","#scheduler = CosineAnnealingLR(optimizer, T_max=20)  # 10 epochs total\n","\n","/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers',device=device, scheduler = scheduler)"],"metadata":{"id":"ooMnu2YhGqaI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742673604246,"user_tz":420,"elapsed":5007697,"user":{"displayName":"David Pham","userId":"09686704807332382354"}},"outputId":"26e38aa5-20f3-4d79-be65-85d6002d60d0"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/36 [00:00<?, ?it/s]<ipython-input-8-12a6e13e4f55>:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","100%|██████████| 36/36 [15:11<00:00, 25.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/20], Train Loss: 2.3777, Val Loss: 1.9190, Val Acc: 0.3951, Val F1: 0.2747\n","Model saved to /content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:41<00:00,  4.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/20], Train Loss: 1.5227, Val Loss: 1.5900, Val Acc: 0.5140, Val F1: 0.4188\n","Model saved to /content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:44<00:00,  4.57s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/20], Train Loss: 1.0697, Val Loss: 1.4549, Val Acc: 0.5542, Val F1: 0.4667\n","Model saved to /content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:42<00:00,  4.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [4/20], Train Loss: 0.7543, Val Loss: 1.4815, Val Acc: 0.5559, Val F1: 0.5060\n","Model saved to /content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:41<00:00,  4.49s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [5/20], Train Loss: 0.5007, Val Loss: 1.3366, Val Acc: 0.6049, Val F1: 0.5272\n","Model saved to /content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:41<00:00,  4.47s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [6/20], Train Loss: 0.3023, Val Loss: 1.3982, Val Acc: 0.6031, Val F1: 0.5472\n","Model saved to /content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:36<00:00,  4.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [7/20], Train Loss: 0.1860, Val Loss: 1.5282, Val Acc: 0.5944, Val F1: 0.5352\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:36<00:00,  4.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [8/20], Train Loss: 0.1614, Val Loss: 1.5913, Val Acc: 0.5839, Val F1: 0.5090\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:38<00:00,  4.41s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [9/20], Train Loss: 0.1411, Val Loss: 1.5104, Val Acc: 0.5892, Val F1: 0.5262\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:39<00:00,  4.43s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [10/20], Train Loss: 0.0830, Val Loss: 1.4835, Val Acc: 0.6224, Val F1: 0.5654\n","Model saved to /content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:41<00:00,  4.48s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [11/20], Train Loss: 0.0516, Val Loss: 1.4726, Val Acc: 0.6259, Val F1: 0.5610\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:36<00:00,  4.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [12/20], Train Loss: 0.0388, Val Loss: 1.4725, Val Acc: 0.5962, Val F1: 0.5418\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:37<00:00,  4.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [13/20], Train Loss: 0.0337, Val Loss: 1.4882, Val Acc: 0.6154, Val F1: 0.5485\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:35<00:00,  4.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [14/20], Train Loss: 0.0332, Val Loss: 1.4689, Val Acc: 0.6381, Val F1: 0.5837\n","Model saved to /content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:38<00:00,  4.41s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [15/20], Train Loss: 0.0341, Val Loss: 1.4563, Val Acc: 0.6206, Val F1: 0.5539\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:37<00:00,  4.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [16/20], Train Loss: 0.0332, Val Loss: 1.4937, Val Acc: 0.6259, Val F1: 0.5589\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:38<00:00,  4.40s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [17/20], Train Loss: 0.0326, Val Loss: 1.4185, Val Acc: 0.6469, Val F1: 0.5841\n","Model saved to /content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:42<00:00,  4.52s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [18/20], Train Loss: 0.0326, Val Loss: 1.5044, Val Acc: 0.6259, Val F1: 0.5678\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:41<00:00,  4.48s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [19/20], Train Loss: 0.0323, Val Loss: 1.4894, Val Acc: 0.6416, Val F1: 0.5841\n","Model saved to /content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 36/36 [02:39<00:00,  4.43s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [20/20], Train Loss: 0.0305, Val Loss: 1.4799, Val Acc: 0.6259, Val F1: 0.5629\n"]}]},{"cell_type":"code","source":["TestModel = models.vit_b_16(weights=\"IMAGENET1K_V1\")\n","#need to change the fc to finetune it\n","num_ftrs = TestModel.heads.head.in_features\n","TestModel.heads.head = nn.Linear(num_ftrs, 21)  # 7 classes for HAM10000\n","\n","# Load the saved weights\n","TestModel.load_state_dict(torch.load('/content/drive/MyDrive/AJL Team 2/VIT_train_ham_first_20epochs_plateulr_patience_2_augmentedset8Layers', map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n","TestModel.to(device)"],"metadata":{"id":"sORLaYgb9eNY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742673607775,"user_tz":420,"elapsed":3544,"user":{"displayName":"David Pham","userId":"09686704807332382354"}},"outputId":"92c9d0ed-e888-4d13-a6b8-3e16917732b2"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VisionTransformer(\n","  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","  (encoder): Encoder(\n","    (dropout): Dropout(p=0.0, inplace=False)\n","    (layers): Sequential(\n","      (encoder_layer_0): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_1): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_2): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_3): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_4): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_5): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_6): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_7): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_8): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_9): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_10): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (encoder_layer_11): EncoderBlock(\n","        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (mlp): MLPBlock(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Dropout(p=0.0, inplace=False)\n","          (3): Linear(in_features=3072, out_features=768, bias=True)\n","          (4): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (heads): Sequential(\n","    (head): Linear(in_features=768, out_features=21, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["#now need to get testing labels to submit\n","class TestDataset(torch.utils.data.Dataset):\n","    def __init__(self, csv_path, root_dir, transform=None):\n","        self.data = pd.read_csv(csv_path)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","        img_hash = row[\"md5hash\"]\n","\n","        img_path = os.path.join(self.root_dir, img_hash + \".jpg\")\n","\n","        if not os.path.exists(img_path):\n","            raise FileNotFoundError(f\"Image {img_hash}.jpg not found in {img_path}\")\n","\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, img_hash  # return hash to map it to a label in submission csv\n","\n","# same transformations as training\n","test_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","])\n","path_test = '/content/drive/MyDrive/AJL Team 2/bttai-ajl-2025'\n","\n","# path to test CSV and test image folder\n","test_csv_path = os.path.join(path_test, 'test.csv')\n","test_root_dir = os.path.join(path_test, 'test', 'test')\n","\n","# load test dataset\n","test_dataset = TestDataset(test_csv_path, test_root_dir, transform=test_transforms)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n","\n","print(f\"Total test samples: {len(test_dataset)}\")"],"metadata":{"id":"VuoFLgTt9uxj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742673608667,"user_tz":420,"elapsed":890,"user":{"displayName":"David Pham","userId":"09686704807332382354"}},"outputId":"1d97f28f-d1b7-4399-e780-6a112cd4e9fb"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Total test samples: 1227\n"]}]},{"cell_type":"code","source":["def predict_and_save(model, test_loader, device, idx_to_class):\n","    model.eval\n","\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for images, img_hashes in tqdm(test_loader, desc=\"Generating Predictions\"):\n","            images = images.to(device)\n","\n","            # predict\n","            outputs = model(images)\n","\n","            # Get predicted class index\n","            _, predicted = torch.max(outputs, 1)\n","\n","            # Convert index to class label\n","            predicted_labels = [idx_to_class[idx.item()] for idx in predicted]\n","\n","            # Store results\n","            for img_hash, label, in zip(img_hashes, predicted_labels):\n","                predictions.append({\"md5hash\": img_hash, \"label\": label})\n","    submission_df = pd.DataFrame(predictions)\n","    return submission_df"],"metadata":{"id":"3J6_AhEh9wMm","executionInfo":{"status":"ok","timestamp":1742673608690,"user_tz":420,"elapsed":12,"user":{"displayName":"David Pham","userId":"09686704807332382354"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["\n","# Map indices to class labels\n","idx_to_class = {idx: cls for cls, idx in dataset_pre.class_to_idx.items()}\n","\n","# Run predictions and save\n","submission_df=predict_and_save(TestModel, test_loader, device, idx_to_class)"],"metadata":{"id":"IGv8c70X93-h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742674103549,"user_tz":420,"elapsed":494857,"user":{"displayName":"David Pham","userId":"09686704807332382354"}},"outputId":"3c4409bc-21e3-4f00-89f5-b93b2f355879"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Generating Predictions: 100%|██████████| 77/77 [08:14<00:00,  6.43s/it]\n"]}]},{"cell_type":"code","source":["submission_df.head()\n","#Save to CSV\n","submission_df.to_csv(path_test + \"/ViTLPreTrainOnHamInLoaderAugmentPatience2AugemntedSetTrain8LayersHAMMALLlessWEightDecat.csv\", index=False)"],"metadata":{"id":"n_8ICHGKOzBl","executionInfo":{"status":"ok","timestamp":1742674103569,"user_tz":420,"elapsed":15,"user":{"displayName":"David Pham","userId":"09686704807332382354"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BhLBcnj_ez3p"},"execution_count":null,"outputs":[]}]}